# Claude Code でセキュリティ試験クイズアプリの MVP を1日で作った話

<!-- category: AI/LLM -->

## 学んだこと
- **Claude Code との壁打ち開発**: 要件定義からコード生成、問題作成、GitHub プッシュまで一連の開発フローを Claude Code と対話しながら進められる。「とにかく MVP を最後まで完成させる」という方針が功を奏し、1日で動くプロダクトができた。
- **AI 生成コンテンツの品質と限界**: Claude Code が生成した試験問題の品質は全体的に高かったが、実際に問題を解いてみると不正確な箇所が見つかった。AI の出力を鵜呑みにせず、人間が目視で検証するプロセスは不可欠。
- **複数 AI のクロスレビュー**: 「Claude Code で作成 → OpenAI Codex でレビュー」という、作成と検証を別の AI に担当させるワークフローが有効だった。Codex はレビュー力に定評があり、厳しい指摘を通じて品質向上につながった。

## 実践内容
1. Claude Code に情報セキュリティマネジメント試験の過去問・シラバスを読み込ませ、出題傾向を分析
2. JSON スキーマを設計し、問題データの構造を定義
3. Claude Code がクイズ問題（40問・3カテゴリ）と解説を JSON 形式で生成
4. バニラ HTML/CSS/JavaScript でブラウザベースのクイズ UI を構築
5. 生成された問題 JSON を OpenAI Codex に投入し、正確性・妥当性をクロスレビュー
6. レビュー結果を反映して問題を修正し、GitHub にプッシュ

## ポイント
- **MVP ファースト**: 問題数を絞り、静的ファイルだけで動くシンプルな構成にしたことで、1日で完成まで到達できた。データベースもフレームワークも不要。
- **JSON スキーマ設計の先行投資**: MVP 段階でもスキーマをきちんと定義しておくことで、将来のスケールアップ（問題数増加、カテゴリ追加、DB 移行）に備えられる。
- **AI のピアレビュー**: 1つの AI に作らせて別の AI にレビューさせる手法は、人間のペアプログラミングに似た効果がある。特に専門知識を要するコンテンツでは、複数の視点を入れることで精度が上がる。

## 感想
「AI と壁打ちしながら1日でプロダクトを完成させる」という体験は、開発の常識を変えるものだと感じた。ただし AI は万能ではなく、特に専門分野の正確性については人間の検証が欠かせない。「作る AI」と「検証する AI」を分けるクロスレビュー方式は、今後の AI 活用における実践的なパターンになると思う。
